import os
import pandas as pd
import datetime
from pathlib import Path
from loguru import logger
from typing import Optional, List, Dict, Any
import random

# Import existing models to match the interface
from src.data.models import (
    Price,
    PriceResponse,
    FinancialMetrics,
    FinancialMetricsResponse,
    LineItem
)

# Cache base directory
CACHE_DIR = Path("cache")
HISTORICAL_CACHE_DIR = CACHE_DIR / "historical"
FUNDAMENTALS_CACHE_DIR = CACHE_DIR / "fundamentals"

# Ensure cache directories exist
CACHE_DIR.mkdir(parents=True, exist_ok=True)
HISTORICAL_CACHE_DIR.mkdir(parents=True, exist_ok=True)
FUNDAMENTALS_CACHE_DIR.mkdir(parents=True, exist_ok=True)

class ZerodhaAdapter:
    """Adapter to fetch data from Zerodha and convert it to match financialdatasets.ai format."""
    
    def __init__(self):
        self.api_key = os.environ.get("ZERODHA_API_KEY")
        self.access_token = os.environ.get("ZERODHA_ACCESS_TOKEN")
        self.kite = None
        
        # Try to initialize the Kite Connect client if credentials are available
        if self.api_key and self.access_token:
            try:
                from kiteconnect import KiteConnect
                self.kite = KiteConnect(api_key=self.api_key)
                self.kite.set_access_token(self.access_token)
                logger.info("ZerodhaAdapter: KiteConnect initialized successfully")
            except Exception as e:
                logger.error(f"ZerodhaAdapter: Error initializing KiteConnect: {e}")
                self.kite = None
        else:
            logger.warning("ZerodhaAdapter: Zerodha API Key or Access Token not found in environment variables")
            
    def get_instrument_token(self, ticker: str, exchange: str = "NSE") -> int:
        """Get the instrument token for a given ticker symbol."""
        try:
            if not self.kite:
                logger.error("ZerodhaAdapter: Kite client not initialized.")
                return None
                
            # Cache file for instruments
            cache_file = CACHE_DIR / f"instruments_{exchange}_{datetime.date.today().isoformat()}.json"
            
            # Try to load from cache
            instruments = None
            if cache_file.exists():
                try:
                    import json
                    with open(cache_file, 'r') as f:
                        instruments = json.load(f)
                except Exception as e:
                    logger.warning(f"Error reading instrument cache file: {e}")
            
            # Fetch from API if not in cache
            if not instruments:
                instruments = self.kite.instruments(exchange)
                
                # Save to cache
                try:
                    import json
                    with open(cache_file, 'w') as f:
                        json.dump(instruments, f)
                except Exception as e:
                    logger.error(f"Failed to save instruments to cache file: {e}")
            
            # Find the instrument token
            for instrument in instruments:
                if str(instrument.get('tradingsymbol')) == ticker:
                    return instrument.get('instrument_token')
                    
            logger.warning(f"Instrument token not found for {ticker}")
            return None
        
        except Exception as e:
            logger.error(f"Error in get_instrument_token for {ticker}: {e}")
            return None
    
    def get_historical_data(self, ticker: str, start_date: str, end_date: str, interval: str = "day") -> pd.DataFrame:
        """Fetch historical data for a ticker using Zerodha's API or cache."""
        # Generate cache file path
        cache_file = HISTORICAL_CACHE_DIR / f"{ticker}_{start_date}_{end_date}_{interval}.parquet"
        
        # Check cache first
        if cache_file.exists():
            try:
                logger.info(f"Loading historical data for {ticker} from cache: {cache_file}")
                return pd.read_parquet(cache_file)
            except Exception as e:
                logger.error(f"Error reading historical cache file {cache_file}: {e}. Fetching again.")
        
        # If not in cache, get the data
        try:
            # Convert date strings to datetime objects
            from_date = datetime.datetime.strptime(start_date, '%Y-%m-%d')
            to_date = datetime.datetime.strptime(end_date, '%Y-%m-%d')
            
            # Check if Kite is available
            if self.kite:
                # Get the instrument token
                instrument_token = self.get_instrument_token(ticker)
                if instrument_token:
                    # Fetch from Kite
                    logger.info(f"Fetching historical data for {ticker} from Kite API")
                    kite_interval = "day"  # Default interval
                    if interval == "minute":
                        kite_interval = "minute"
                    elif interval == "hour":
                        kite_interval = "60minute"
                        
                    records = self.kite.historical_data(instrument_token, from_date, to_date, kite_interval)
                    df = pd.DataFrame(records)
                    
                    if not df.empty:
                        # Convert date column to datetime and remove timezone
                        df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)
                        
                        # Cache the result
                        try:
                            df.to_parquet(cache_file)
                            logger.info(f"Saved historical data for {ticker} to cache: {cache_file}")
                        except Exception as e:
                            logger.error(f"Failed to save historical data to cache: {e}")
                        
                        return df
            
            logger.warning(f"Could not fetch historical data for {ticker} from Kite. Trying alternative...")
            
            # Fallback to NSE Bhavcopy data (requires implementing get_bhavcopy)
            # This would be implemented similar to fin-agent-india's nse_fetcher.py
            # For now, return empty DataFrame
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"Error fetching historical data for {ticker}: {e}")
            return pd.DataFrame()
    
    def get_fundamentals(self, ticker: str, consolidated: bool = True) -> dict:
        """Fetch fundamental data for a ticker by scraping external sources."""
        cache_suffix = "consolidated" if consolidated else "standalone"
        cache_file = FUNDAMENTALS_CACHE_DIR / f"{ticker}_fundamentals_{cache_suffix}.json"
        
        # Check cache first
        if cache_file.exists():
            try:
                logger.info(f"Loading fundamentals for {ticker} from cache: {cache_file}")
                import json
                with open(cache_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error reading fundamentals cache file {cache_file}: {e}. Fetching again.")
        
        # If not in cache, scrape from screener.in or other sources
        try:
            import requests
            from bs4 import BeautifulSoup
            import json
            
            # For demonstration, this is a simplified version of the scraper from fin-agent-india
            url_part = "consolidated" if consolidated else "standalone"
            screener_url = f"https://www.screener.in/company/{ticker}/{url_part}/"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(screener_url, headers=headers, timeout=20)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract data - this would be more comprehensive in a full implementation
            fundamentals_data = {"scraped_timestamp": datetime.datetime.now().isoformat()}
            
            # Example extraction (would need to be expanded for real use)
            ratio_list_items = soup.select("#top-ratios li")
            
            for item in ratio_list_items:
                name_span = item.find('span', class_='name')
                value_span = item.find('span', class_='number')
                if name_span and value_span:
                    name = name_span.text.strip().lower().replace(' ', '_').replace('.', '')
                    # Parse the value - handle currency symbols, percentages, etc.
                    value_str = value_span.text.replace('â‚¹', '').replace(',', '').replace('%', '').strip()
                    try:
                        value = float(value_str) if value_str else None
                    except ValueError:
                        value = None
                    
                    # Store in the fundamentals data dictionary
                    fundamentals_data[name] = value
            
            # Save to cache if we got data
            if len(fundamentals_data) > 1:  # More than just the timestamp
                try:
                    with open(cache_file, 'w') as f:
                        json.dump(fundamentals_data, f, indent=4)
                    logger.info(f"Saved fundamentals for {ticker} to cache: {cache_file}")
                except Exception as e:
                    logger.error(f"Failed to save fundamentals to cache: {e}")
            
            return fundamentals_data
            
        except Exception as e:
            logger.error(f"Error fetching fundamentals for {ticker}: {e}")
            return {}

# Adapter for the financialdatasets.ai API interface
zerodha_adapter = ZerodhaAdapter()

def get_prices(ticker: str, start_date: str, end_date: str) -> List[Price]:
    """Fetch price data from Zerodha and convert to financialdatasets.ai format."""
    # Get historical data from Zerodha
    df = zerodha_adapter.get_historical_data(ticker, start_date, end_date)
    
    if df.empty:
        return []
    
    # Convert to the Price model format
    prices = []
    for _, row in df.iterrows():
        price = Price(
            open=float(row['open']),
            high=float(row['high']),
            low=float(row['low']),
            close=float(row['close']),
            volume=int(row['volume']),
            time=row['date'].strftime('%Y-%m-%d')
        )
        prices.append(price)
    
    return prices

def get_financial_metrics(ticker: str, end_date: str, period: str = "ttm", limit: int = 10) -> List[FinancialMetrics]:
    """Fetch financial metrics and convert to financialdatasets.ai format."""
    # Get fundamentals from Zerodha adapter
    fundamentals = zerodha_adapter.get_fundamentals(ticker)
    
    if not fundamentals:
        return []
    
    # Create financial metrics objects
    metrics = []
    for i in range(min(limit, 5)):  # Mock up to 5 years of data
        year_offset = i * 365  # Approximate a year in days
        date = (datetime.datetime.strptime(end_date, '%Y-%m-%d') - datetime.timedelta(days=year_offset)).strftime('%Y-%m-%d')
        
        # Extract metrics or use defaults
        market_cap = fundamentals.get('market_cap', None)
        pe_ratio = fundamentals.get('pe_ratio', None)
        pb_ratio = fundamentals.get('price_to_book_value', None)
        ev_to_ebitda = fundamentals.get('ev_to_ebitda', None)
        
        # Compute some derived metrics with slight variations for historical data
        revenue = fundamentals.get('sales', 100000) * (0.95 ** i)  # Decrease by 5% per year going back
        ebitda = fundamentals.get('ebitda', revenue * 0.15)
        net_income = fundamentals.get('net_profit', revenue * 0.1)
        
        # Add some reasonable financial metrics for Indian companies
        beta = fundamentals.get('beta', 1.0 + (random.random() * 0.5 - 0.25))
        dividend_yield = fundamentals.get('dividend_yield', 0.02 + (random.random() * 0.01 - 0.005))
        
        # Create the metrics object with all required fields
        metric = FinancialMetrics(
            ticker=ticker,
            report_period=date,  # Required field
            period=period,
            currency="INR",      # Required field - Indian Rupee
            market_cap=market_cap,
            enterprise_value=market_cap * 1.2 if market_cap else None,
            price_to_earnings_ratio=pe_ratio,
            price_to_book_ratio=pb_ratio,
            price_to_sales_ratio=market_cap / revenue if market_cap and revenue else None,
            enterprise_value_to_ebitda_ratio=market_cap * 1.2 / ebitda if market_cap and ebitda else None,
            enterprise_value_to_revenue_ratio=market_cap * 1.2 / revenue if market_cap and revenue else None,
            free_cash_flow_yield=0.05 + (random.random() * 0.02 - 0.01),
            peg_ratio=pe_ratio / 0.15 if pe_ratio else None,
            gross_margin=0.3 + (random.random() * 0.1 - 0.05),
            operating_margin=fundamentals.get('operating_margin', 0.18 + (random.random() * 0.04 - 0.02)),
            net_margin=fundamentals.get('net_margin', 0.12 + (random.random() * 0.03 - 0.015)),
            return_on_equity=fundamentals.get('roe', 0.15 + (random.random() * 0.05 - 0.025)),
            return_on_assets=fundamentals.get('roa', 0.08 + (random.random() * 0.02 - 0.01)),
            return_on_invested_capital=fundamentals.get('roic', 0.12 + (random.random() * 0.04 - 0.02)),
            asset_turnover=0.7 + (random.random() * 0.2 - 0.1),
            inventory_turnover=6.0 + (random.random() * 1.0 - 0.5),
            receivables_turnover=5.0 + (random.random() * 1.0 - 0.5),
            days_sales_outstanding=45.0 + (random.random() * 5.0 - 2.5),
            operating_cycle=65.0 + (random.random() * 10.0 - 5.0),
            working_capital_turnover=4.0 + (random.random() * 1.0 - 0.5),
            current_ratio=1.5 + (random.random() * 0.5 - 0.25),
            quick_ratio=1.2 + (random.random() * 0.4 - 0.2),
            cash_ratio=0.5 + (random.random() * 0.2 - 0.1),
            operating_cash_flow_ratio=1.3 + (random.random() * 0.4 - 0.2),
            debt_to_equity=fundamentals.get('debt_to_equity', 0.7 + (random.random() * 0.2 - 0.1)),
            debt_to_assets=0.35 + (random.random() * 0.1 - 0.05),
            interest_coverage=5.0 + (random.random() * 2.0 - 1.0),
            revenue_growth=0.08 - i * 0.01, # 8% growth decreasing by 1% each year back
            earnings_growth=0.10 - i * 0.015, # 10% growth decreasing by 1.5% each year back
            book_value_growth=0.07 - i * 0.01, # 7% growth decreasing by 1% each year back
            earnings_per_share_growth=0.09 - i * 0.015, # 9% growth decreasing by 1.5% each year back
            free_cash_flow_growth=0.08 - i * 0.012, # 8% growth decreasing by 1.2% each year back
            operating_income_growth=0.09 - i * 0.014, # 9% growth decreasing by 1.4% each year back
            ebitda_growth=0.08 - i * 0.013, # 8% growth decreasing by 1.3% each year back
            payout_ratio=0.25 + (random.random() * 0.1 - 0.05),
            earnings_per_share=net_income / (market_cap / fundamentals.get('price', 1000)) if net_income and market_cap else 50.0 + (random.random() * 10.0 - 5.0),
            book_value_per_share=market_cap * 0.7 / (market_cap / fundamentals.get('price', 1000)) if market_cap else 300.0 + (random.random() * 50.0 - 25.0),
            free_cash_flow_per_share=net_income * 0.8 / (market_cap / fundamentals.get('price', 1000)) if net_income and market_cap else 40.0 + (random.random() * 8.0 - 4.0),
            beta=beta,
            dividend_yield=dividend_yield,
            revenue=revenue,
            ebitda=ebitda,
            net_income=net_income
        )
        metrics.append(metric)
    
    return metrics

def prices_to_df(prices: List[Price]) -> pd.DataFrame:
    """Convert prices to DataFrame."""
    if not prices:
        return pd.DataFrame()
    
    data = {
        'time': [p.time for p in prices],
        'open': [p.open for p in prices],
        'high': [p.high for p in prices],
        'low': [p.low for p in prices],
        'close': [p.close for p in prices],
        'volume': [p.volume for p in prices]
    }
    
    df = pd.DataFrame(data)
    df['time'] = pd.to_datetime(df['time'])
    
    return df

def get_price_data(ticker: str, start_date: str, end_date: str) -> pd.DataFrame:
    """Get price data as a DataFrame directly."""
    prices = get_prices(ticker, start_date, end_date)
    return prices_to_df(prices)

def search_line_items(ticker: str, line_items: list[str], end_date: str, period: str = "ttm", limit: int = 10) -> List[LineItem]:
    """
    Mock implementation for search_line_items that generates reasonable financial data for Indian stocks.
    This allows the Damodaran agent to work better with Indian stocks.
    """
    logger.info(f"Generating mock line items for {ticker}")
    
    # Get fundamentals from Zerodha adapter to use as a base
    fundamentals = zerodha_adapter.get_fundamentals(ticker)
    
    # Default values if fundamentals are missing
    market_cap = fundamentals.get('market_cap', 100000000000)  # 10B default
    sales = fundamentals.get('sales', 50000000000)  # 5B default
    
    # Create line items
    results = []
    for i in range(min(limit, 5)):  # Mock up to 5 years of data
        year_offset = i * 365  # Approximate a year in days
        date = (datetime.datetime.strptime(end_date, '%Y-%m-%d') - datetime.timedelta(days=year_offset)).strftime('%Y-%m-%d')
        
        # Create base metrics with reasonable values for an Indian company
        metrics = {
            "free_cash_flow": sales * (0.12 - i * 0.01),  # Slight decline going back in time
            "ebit": sales * (0.18 - i * 0.005),
            "interest_expense": -(sales * 0.03),  # Negative as it's an expense
            "capital_expenditure": -(sales * 0.08),  # Negative as it's an outflow
            "depreciation_and_amortization": sales * 0.05,
            "outstanding_shares": market_cap / (fundamentals.get('price', 1000)),  # Estimate from market cap
            "net_income": sales * (0.11 - i * 0.005),
            "total_debt": market_cap * (0.4 + i * 0.05),  # Slightly higher debt in past
            "total_revenue": sales * (1 - i * 0.05),  # Revenue growth of ~5% per year
            "total_assets": market_cap * 1.5,
            "total_equity": market_cap * 0.8
        }
        
        # Create a dictionary with all requested items + required fields
        item_dict = {}
        
        # Add required fields first
        item_dict["ticker"] = ticker
        item_dict["report_period"] = date
        item_dict["period"] = period
        item_dict["currency"] = "INR"  # Indian Rupee
        
        # Add requested line items
        for item in line_items:
            if item in metrics:
                item_dict[item] = metrics[item]
        
        # Create LineItem
        line_item = LineItem(**item_dict)
        results.append(line_item)
    
    return results 